{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdOP4RVmWHRm",
    "outputId": "642cac53-b878-4ebc-add6-6103795028c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UCL_JQUTWQ9t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1beMRpJWo4U",
    "outputId": "376441b9-eb07-474c-ce34-375046e167c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xVZEX9DHOKgp"
   },
   "outputs": [],
   "source": [
    "path_name = '/content/gdrive/MyDrive/11785/idl-fall2021-hw1p2'\n",
    "#train_data = np.load(os.path.join(path_name,'train.npy'),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTQN-OkMTYAa"
   },
   "outputs": [],
   "source": [
    "train_data = np.load(os.path.join(path_name,'train.npy'),allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6HWB-q-TmVr",
    "outputId": "c2021e0c-ec46-4bad-d567-a1b4bdd7c2c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1184,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXwWPDtLV7BI",
    "outputId": "12fc69ef-b494-4619-aa52-9aa567db872e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1184"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Vked7uYvWtNC"
   },
   "outputs": [],
   "source": [
    "class MySpeechDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,data,k):\n",
    "    self.x = data[0]\n",
    "    self.y = data[1]\n",
    "    self.k = k\n",
    "    index_map_x = []\n",
    "    for i,x_data in enumerate(self.x):\n",
    "      for j in range(x_data.shape[0]):\n",
    "        index_pairs = (i,j)\n",
    "        index_map_x.append(index_pairs)\n",
    "    self.index_map = index_map_x    \n",
    "    self.length = len(self.index_map)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.index_map)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    i, j = self.index_map[index]\n",
    "    ylabel = self.y[i][j] \n",
    "    x_with_context = self.x[i].take(range(j-self.k,j+self.k+1),axis=0,mode='clip').reshape(-1)\n",
    "    xfeature = torch.from_numpy(x_with_context).float()\n",
    "    return xfeature, ylabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZJLI5FAbv_vE"
   },
   "outputs": [],
   "source": [
    "class MySpeechDatasettest(torch.utils.data.Dataset):\n",
    "  def __init__(self,data,k):\n",
    "    self.x = data\n",
    "    self.k = k\n",
    "    index_map_x = []\n",
    "    for i,x_data in enumerate(self.x):\n",
    "      for j in range(x_data.shape[0]):\n",
    "        index_pairs = (i,j)\n",
    "        index_map_x.append(index_pairs)\n",
    "    self.index_map = index_map_x    \n",
    "    self.length = len(self.index_map)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.index_map)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    i, j = self.index_map[index]\n",
    "    x_with_context = self.x[i].take(range(j-self.k,j+self.k+1),axis=0,mode='clip').reshape(-1)\n",
    "    xfeature = torch.from_numpy(x_with_context).float()\n",
    "    return xfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YXNh8sXiMCgS"
   },
   "outputs": [],
   "source": [
    "num_workers = 4 if cuda else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2hPgQgm1MU3d"
   },
   "outputs": [],
   "source": [
    "#training\n",
    "data_train = (np.load(os.path.join(path_name,'train.npy'),allow_pickle=True), np.load(os.path.join(path_name,'train_labels.npy'),allow_pickle=True))\n",
    "train_dataset = MySpeechDataset(data_train, k=20)\n",
    "train_loader_args = dict(shuffle=True, batch_size=256, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=32)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args)\n",
    "\n",
    "#data_validation = (np.load(os.path.join(path_name,'dev.npy'),allow_pickle=True), np.load(os.path.join(path_name,'dev_labels.npy'),allow_pickle=True))\n",
    "#validation_dataset = MySpeechDataset(data_validation, k=20)\n",
    "#validation_loader_args = dict(shuffle=False, batch_size=32, num_workers=num_workers)\n",
    "#validation_loader = DataLoader(validation_dataset, **validation_loader_args)\n",
    "\n",
    "data_test = np.load(os.path.join(path_name,'test.npy'),allow_pickle=True)\n",
    "test_dataset = MySpeechDatasettest(data_test, k=20)\n",
    "test_loader_args = dict(shuffle=False, batch_size=32, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, **test_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1Xy6JkTL0zBr"
   },
   "outputs": [],
   "source": [
    "data_validation = (np.load(os.path.join(path_name,'dev.npy'),allow_pickle=True), np.load(os.path.join(path_name,'dev_labels.npy'),allow_pickle=True))\n",
    "validation_dataset = MySpeechDataset(data_validation, k=20)\n",
    "validation_loader_args = dict(shuffle=False, batch_size=32, num_workers=num_workers)\n",
    "validation_loader = DataLoader(validation_dataset, **validation_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "W1EZYkZ2ac3s"
   },
   "outputs": [],
   "source": [
    "class SpeechMLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SpeechMLP, self).__init__()\n",
    "    self.model = nn.Sequential(nn.Linear(1640,2048), nn.BatchNorm1d(2048), nn.LeakyReLU(), nn.Linear(2048,1024), nn.BatchNorm1d(1024), nn.LeakyReLU(), nn.Linear(1024,1024), nn.BatchNorm1d(1024), nn.LeakyReLU(), nn.Linear(1024,1024), nn.BatchNorm1d(1024), nn.LeakyReLU(), nn.Linear(1024,1024), nn.BatchNorm1d(1024), nn.LeakyReLU(), nn.Linear(1024,1024), nn.BatchNorm1d(1024), nn.LeakyReLU(), nn.Linear(1024,512),nn.LeakyReLU(), nn.Linear(512,256),nn.LeakyReLU(), nn.Linear(256,71))\n",
    "  def forward(self,x):\n",
    "    return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tjwa1mEZ0bNu",
    "outputId": "24079e93-aa58-4572-8395-dd93bc819f5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechMLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1640, out_features=2048, bias=True)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.01)\n",
      "    (9): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (10): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (13): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.01)\n",
      "    (15): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (16): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): LeakyReLU(negative_slope=0.01)\n",
      "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01)\n",
      "    (20): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (21): LeakyReLU(negative_slope=0.01)\n",
      "    (22): Linear(in_features=256, out_features=71, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SpeechMLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tS45dvkz1exw"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   \n",
    "        data = data.to(device)\n",
    "        target = target.type(torch.LongTensor)\n",
    "        target = target.to(device) \n",
    "        \n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kS7JH1YUPFoG"
   },
   "outputs": [],
   "source": [
    "def validation_model(model, validation_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(validation_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.type(torch.LongTensor)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        running_loss /= len(validation_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Validation Loss: ', running_loss)\n",
    "        print('Validation Accuracy: ', acc, '%')\n",
    "        return running_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wAyiXOKf7GLx"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        predictions = []\n",
    "        for batch_idx, (data) in enumerate(test_loader):   \n",
    "            data = data.to(device)\n",
    "            #target = target.type(torch.LongTensor)\n",
    "            #target = target.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.append(predicted)\n",
    "            \n",
    "        \n",
    "        predictions = torch.cat(predictions, axis=0).cpu().numpy()\n",
    "        #predictions = np.array(list(predictions))\n",
    "        num_predictions = np.arange(0,len(predictions))\n",
    "        df = pd.DataFrame()\n",
    "        df['id'] = num_predictions\n",
    "        df['label'] = predictions\n",
    "        df.to_csv(os.path.join(path_name,'predictions.csv'),index=False)\n",
    "\n",
    "        \n",
    "        #running_loss /= len(test_loader)\n",
    "        #acc = (correct_predictions/total_predictions)*100.0\n",
    "        #print('Testing Loss: ', running_loss)\n",
    "        #print('Testing Accuracy: ', acc, '%')\n",
    "        #return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbs3WDHtMNr8",
    "outputId": "12643d78-4cc9-4027-b69e-f91f636f0a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.8637536383286077 Time:  2348.9291043281555 s\n",
      "====================\n",
      "Training Loss:  0.5987557614687572 Time:  2347.7928614616394 s\n",
      "====================\n",
      "Training Loss:  0.5078717617570858 Time:  2355.4739983081818 s\n",
      "====================\n",
      "Training Loss:  0.45006239256163716 Time:  2349.14657497406 s\n",
      "====================\n",
      "Training Loss:  0.407503372262183 Time:  2345.305848121643 s\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "Train_loss = []\n",
    "#Validation_loss = []\n",
    "#Validation_acc = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    #validation_loss, validation_acc = validation_model(model, validation_loader, criterion)\n",
    "    #test_loss, test_acc = test_model(model, test_loader, criterion)\n",
    "    Train_loss.append(train_loss)\n",
    "    #Validation_loss.append(validation_loss)\n",
    "    #Validation_acc.append(validation_acc)\n",
    "    #Test_loss.append(test_loss)\n",
    "    #Test_acc.append(test_acc)\n",
    "    print('='*20)\n",
    "    model_path = os.path.join(path_name, 'model.pt')\n",
    "    torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZsSlq8v06AT",
    "outputId": "3f35e6ca-3264-42f9-da5d-180c300feba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  0.9393573400696338\n",
      "Validation Accuracy:  75.22427646462283 %\n"
     ]
    }
   ],
   "source": [
    "validation_loss, validation_acc = validation_model(model, validation_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RJDtu36-XUSs"
   },
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(path_name, 'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snQ4WO9Q1FzO"
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(path_name, 'model.pt')\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lbsw4mEeqbn0"
   },
   "outputs": [],
   "source": [
    "test_model(model, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "HW1_P2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
